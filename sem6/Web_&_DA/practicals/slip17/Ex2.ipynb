{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 2)Consider text paragraph.So, keep working. Keep striving. Never give up. Fall down seven times, get \n",
    "up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than \n",
    "hardship. So, keep moving, keep growing, keep learning. See you at work.Preprocess the text to remove \n",
    "any special characters and digits. Generate the summary using extractive summarization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ease is a greater threat to progress than hardship.Ease is a greater threat to progress than \\nhardship.So, keep moving, keep growing, keep learning. o, keep working.Keep striving.Fall down seven times, get \\nup eight.Never give up.See you at work'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re \n",
    "\n",
    "def get_process_text(text):\n",
    "    process_text= re.sub(r'[^a-zA-Z\\s]' , '' , text)\n",
    "    process_text = re.sub(r'\\d+','',process_text)\n",
    "    return process_text\n",
    "\n",
    "text = \"\"\" o, keep working. Keep striving. Never give up. Fall down seven times, get \n",
    "up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than \n",
    "hardship. So, keep moving, keep growing, keep learning. See you at work\"\"\"\n",
    "\n",
    "process_text = get_process_text(text)\n",
    "\n",
    "# get all the stop words \n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(process_text)\n",
    "\n",
    "# count the frequency of each word\n",
    "word_freq ={}\n",
    "for word in words:\n",
    "    if word in stopWords:\n",
    "        continue \n",
    "    if word in word_freq:\n",
    "        word_freq[word]+=1 \n",
    "    else:\n",
    "        word_freq[word] = 1 \n",
    "\n",
    "maximum_frq  = max(word_freq.values())\n",
    "\n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/maximum_frq)\n",
    "\n",
    "# tokenize the sentence\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "sentence_values = {}\n",
    "for sentence in sentences:\n",
    "    for word,frq in word_freq.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentence_values:\n",
    "                sentence_values[sentence]+= frq \n",
    "            else:\n",
    "                sentence_values[sentence] = frq \n",
    "\n",
    "import heapq \n",
    "\n",
    "summary = heapq.nlargest(10 , sentence_values , key = sentence_values.get)\n",
    "summary = ''.join(summary)\n",
    "summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
