{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 2 )Consider text paragraph. \"\"\"Hello all, Welcome to Python Programming Academy. Python \n",
    "Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled \n",
    "in this Academy.\"\"\"  Preprocess the text to remove any special characters and digits. Generate the \n",
    "summary using extractive summarization  process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled in this Academy. Hello all, Welcome to Python Programming Academy.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
    "import re\n",
    "\n",
    "def get_preprocess_text(text):\n",
    "    processed_text = re.sub(r'[^a-zA-Z\\s]' , '' ,text)\n",
    "    processed_text = re.sub(r'\\d+' ,'',processed_text)\n",
    "    return processed_text\n",
    "\n",
    "text =\"\"\"Hello all, Welcome to Python Programming Academy. Python Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled in this Academy.\"\"\"\n",
    "processed_text = get_preprocess_text(text)\n",
    "processed_text\n",
    "\n",
    "stopWords  = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = word_tokenize(processed_text)\n",
    "\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word in stopWords:\n",
    "        continue\n",
    "    if word in word_freq:\n",
    "        word_freq[word] +=1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "maximum_frq = max(word_freq.values())\n",
    "\n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/maximum_frq)\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentenceValues = {}\n",
    "for sentence in sentences:\n",
    "    for word,frq in word_freq.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentenceValues:\n",
    "                sentenceValues[sentence] += frq \n",
    "            else:\n",
    "                sentenceValues[sentence] = frq \n",
    "\n",
    "\n",
    "import heapq \n",
    "summary = ''\n",
    "summary_sentences = heapq.nlargest(10, sentenceValues , key = sentenceValues.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
